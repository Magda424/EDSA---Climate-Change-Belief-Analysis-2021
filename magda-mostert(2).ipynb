{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-10T08:14:49.460599Z","iopub.execute_input":"2021-10-10T08:14:49.461386Z","iopub.status.idle":"2021-10-10T08:14:49.472339Z","shell.execute_reply.started":"2021-10-10T08:14:49.461355Z","shell.execute_reply":"2021-10-10T08:14:49.471453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# storing and analysis\nimport numpy as np\nimport pandas as pd\nimport re\n\n# visualization\nimport matplotlib.pyplot as plt\nimport warnings\nimport nltk\nimport string\nimport seaborn as sns\n\n#import text classification modules\nimport os\nfrom nltk.tokenize import WordPunctTokenizer\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\n\nfrom nltk.stem.porter import * \nfrom wordcloud import WordCloud\nimport spacy\nfrom spacy import displacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# import train/test split module\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n# import scoring metrice\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# suppress cell warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nos.listdir('../input/edsa-climate-change-belief-analysis-2021')","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:14:54.074289Z","iopub.execute_input":"2021-10-10T08:14:54.074712Z","iopub.status.idle":"2021-10-10T08:14:54.092961Z","shell.execute_reply.started":"2021-10-10T08:14:54.074681Z","shell.execute_reply":"2021-10-10T08:14:54.091897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain_df = pd.read_csv('../input/edsa-climate-change-belief-analysis-2021/train.csv')\ntest_df = pd.read_csv('../input/edsa-climate-change-belief-analysis-2021/test.csv')\ntrain_df.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:00.637413Z","iopub.execute_input":"2021-10-10T08:15:00.637966Z","iopub.status.idle":"2021-10-10T08:15:00.726011Z","shell.execute_reply.started":"2021-10-10T08:15:00.637934Z","shell.execute_reply":"2021-10-10T08:15:00.725379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:05.693877Z","iopub.execute_input":"2021-10-10T08:15:05.694431Z","iopub.status.idle":"2021-10-10T08:15:05.703644Z","shell.execute_reply.started":"2021-10-10T08:15:05.694399Z","shell.execute_reply":"2021-10-10T08:15:05.702525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print out the Shape of the training data and the testing data\nprint(train_df.shape)\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:09.84893Z","iopub.execute_input":"2021-10-10T08:15:09.849219Z","iopub.status.idle":"2021-10-10T08:15:09.855349Z","shell.execute_reply.started":"2021-10-10T08:15:09.849189Z","shell.execute_reply":"2021-10-10T08:15:09.853965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print out the Shape of the training data and the testing data\ntrain_df['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:13.471003Z","iopub.execute_input":"2021-10-10T08:15:13.471616Z","iopub.status.idle":"2021-10-10T08:15:13.48108Z","shell.execute_reply.started":"2021-10-10T08:15:13.471573Z","shell.execute_reply":"2021-10-10T08:15:13.479909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Confirm if the dataset have any null values\ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:17.110778Z","iopub.execute_input":"2021-10-10T08:15:17.111075Z","iopub.status.idle":"2021-10-10T08:15:17.123027Z","shell.execute_reply.started":"2021-10-10T08:15:17.111045Z","shell.execute_reply":"2021-10-10T08:15:17.121907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Combining both train and test data set before data cleaning \ndata = train_df.append(test_df, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:20.081656Z","iopub.execute_input":"2021-10-10T08:15:20.082007Z","iopub.status.idle":"2021-10-10T08:15:20.090545Z","shell.execute_reply.started":"2021-10-10T08:15:20.081976Z","shell.execute_reply":"2021-10-10T08:15:20.089738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:26.108737Z","iopub.execute_input":"2021-10-10T08:15:26.109035Z","iopub.status.idle":"2021-10-10T08:15:26.114185Z","shell.execute_reply.started":"2021-10-10T08:15:26.109005Z","shell.execute_reply":"2021-10-10T08:15:26.113177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove twitter handle and converting to lower case\n\ndata['tidy_message'] = np.vectorize(remove_pattern)(data['message'], \"@[\\w]*\")\ndata['tidy_message'] = data['tidy_message'].str.replace('RT :',' ')\ndata['tidy_message'] = data['tidy_message'].apply(lambda x: x.lower())\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:28.963882Z","iopub.execute_input":"2021-10-10T08:15:28.964157Z","iopub.status.idle":"2021-10-10T08:15:30.64208Z","shell.execute_reply.started":"2021-10-10T08:15:28.964129Z","shell.execute_reply":"2021-10-10T08:15:30.641051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing short words  - can extract more meaning out of data\n\nshort_word_dict = {\n\"121\": \"one to one\",\n\"a/s/l\": \"age, sex, location\",\n\"adn\": \"any day now\",\n\"afaik\": \"as far as I know\",\n\"afk\": \"away from keyboard\",\n\"aight\": \"alright\",\n\"alol\": \"actually laughing out loud\",\n\"b4\": \"before\",\n\"b4n\": \"bye for now\",\n\"bak\": \"back at the keyboard\",\n\"bf\": \"boyfriend\",\n\"bff\": \"best friends forever\",\n\"bfn\": \"bye for now\",\n\"bg\": \"big grin\",\n\"bta\": \"but then again\",\n\"btw\": \"by the way\",\n\"cid\": \"crying in disgrace\",\n\"cnp\": \"continued in my next post\",\n\"cp\": \"chat post\",\n\"cu\": \"see you\",\n\"cul\": \"see you later\",\n\"cul8r\": \"see you later\",\n\"cya\": \"bye\",\n\"cyo\": \"see you online\",\n\"dbau\": \"doing business as usual\",\n\"fud\": \"fear, uncertainty, and doubt\",\n\"fwiw\": \"for what it's worth\",\n\"fyi\": \"for your information\",\n\"g\": \"grin\",\n\"g2g\": \"got to go\",\n\"ga\": \"go ahead\",\n\"gal\": \"get a life\",\n\"gf\": \"girlfriend\",\n\"gfn\": \"gone for now\",\n\"gmbo\": \"giggling my butt off\",\n\"gmta\": \"great minds think alike\",\n\"h8\": \"hate\",\n\"hagn\": \"have a good night\",\n\"hdop\": \"help delete online predators\",\n\"hhis\": \"hanging head in shame\",\n\"iac\": \"in any case\",\n\"ianal\": \"I am not a lawyer\",\n\"ic\": \"I see\",\n\"idk\": \"I don't know\",\n\"imao\": \"in my arrogant opinion\",\n\"imnsho\": \"in my not so humble opinion\",\n\"imo\": \"in my opinion\",\n\"iow\": \"in other words\",\n\"ipn\": \"I’m posting naked\",\n\"irl\": \"in real life\",\n\"jk\": \"just kidding\",\n\"l8r\": \"later\",\n\"ld\": \"later, dude\",\n\"ldr\": \"long distance relationship\",\n\"llta\": \"lots and lots of thunderous applause\",\n\"lmao\": \"laugh my ass off\",\n\"lmirl\": \"let's meet in real life\",\n\"lol\": \"laugh out loud\",\n\"ltr\": \"longterm relationship\",\n\"lulab\": \"love you like a brother\",\n\"lulas\": \"love you like a sister\",\n\"luv\": \"love\",\n\"m/f\": \"male or female\",\n\"m8\": \"mate\",\n\"milf\": \"mother I would like to fuck\",\n\"oll\": \"online love\",\n\"omg\": \"oh my god\",\n\"otoh\": \"on the other hand\",\n\"pir\": \"parent in room\",\n\"ppl\": \"people\",\n\"r\": \"are\",\n\"rofl\": \"roll on the floor laughing\",\n\"rpg\": \"role playing games\",\n\"ru\": \"are you\",\n\"shid\": \"slaps head in disgust\",\n\"somy\": \"sick of me yet\",\n\"sot\": \"short of time\",\n\"thanx\": \"thanks\",\n\"thx\": \"thanks\",\n\"ttyl\": \"talk to you later\",\n\"u\": \"you\",\n\"ur\": \"you are\",\n\"uw\": \"you’re welcome\",\n\"wb\": \"welcome back\",\n\"wfm\": \"works for me\",\n\"wibni\": \"wouldn't it be nice if\",\n\"wtf\": \"what the fuck\",\n\"wtg\": \"way to go\",\n\"wtgp\": \"want to go private\",\n\"ym\": \"young man\",\n\"gr8\": \"great\",\n\"8yo\":\"eight year old\"\n}","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:36.745963Z","iopub.execute_input":"2021-10-10T08:15:36.746269Z","iopub.status.idle":"2021-10-10T08:15:36.760533Z","shell.execute_reply.started":"2021-10-10T08:15:36.746239Z","shell.execute_reply":"2021-10-10T08:15:36.759593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lookup_dict(text, dictionary):\n    for word in text.split(): #split the text into a list and loop through the list for find the words\n        if word.lower() in dictionary: #Lower the words and see if they are in the dictionary\n            if word.lower() in text.split(): #lower the words and see if they are in the text list\n                text = text.replace(word, dictionary[word.lower()]) #replace the word in the text split with the values in the dictionary\n    return text\n\n\n#Perform lookup of short words and return the string with the full meaning\ndata['tidy_message'] = data['tidy_message'].apply(lambda x: lookup_dict(x,short_word_dict))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:45.564435Z","iopub.execute_input":"2021-10-10T08:15:45.564705Z","iopub.status.idle":"2021-10-10T08:15:45.691929Z","shell.execute_reply.started":"2021-10-10T08:15:45.564676Z","shell.execute_reply":"2021-10-10T08:15:45.691168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contractions = {\n\"ain't\": \"am not / are not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is\",\n\"i'd\": \"I had / I would\",\n\"i'd've\": \"I would have\",\n\"i'll\": \"I shall / I will\",\n\"i'll've\": \"I shall have / I will have\",\n\"i'm\": \"I am\",\n\"i've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:49.647589Z","iopub.execute_input":"2021-10-10T08:15:49.648193Z","iopub.status.idle":"2021-10-10T08:15:49.663411Z","shell.execute_reply.started":"2021-10-10T08:15:49.64814Z","shell.execute_reply":"2021-10-10T08:15:49.6627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n#Perform lookup of apostrophe words and return the string with the full meaning\ndata['tidy_message'] = data['tidy_message'].apply(lambda x: lookup_dict(x,contractions))# Use the contraction dictionary to replace the aprostophe words in data with the full meaning\ndata.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:15:56.353262Z","iopub.execute_input":"2021-10-10T08:15:56.35381Z","iopub.status.idle":"2021-10-10T08:15:56.506167Z","shell.execute_reply.started":"2021-10-10T08:15:56.353777Z","shell.execute_reply":"2021-10-10T08:15:56.505421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Emoji Lookup\n\ntweets_text = data.tidy_message.str.cat() #Concatenate the tweet string and assign the variable 'tweets_text'\nemos = set(re.findall(r\" ([xX:;][-']?.) \",tweets_text)) #Create a regular expression that finds the emojis in the tweets\nemos_count = [] #Initialise an empty list 'emo_count'\nfor emo in emos: #Loop through emoji\n    emos_count.append((tweets_text.count(emo), emo)) #Add emojis found into the emo_count list\nsorted(emos_count,reverse=True) #Sort the list in reverse order\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:16:01.040308Z","iopub.execute_input":"2021-10-10T08:16:01.040845Z","iopub.status.idle":"2021-10-10T08:16:01.142639Z","shell.execute_reply.started":"2021-10-10T08:16:01.040813Z","shell.execute_reply":"2021-10-10T08:16:01.141834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emoticon_dict={\n'XD':'happy',\n';)':'happy',\n':-)':'happy',\n';-)':'happy',\n':P':'happy', \n':)':'happy',\n'x ':'happy',\n':(':'sad',\n':/':'sad'\n}\n\n\n\n#Lookup the emojis in the tweets and replace them with the meaning behind the tweet\ndata['tidy_message'] = data['tidy_message'].apply(lambda x: lookup_dict(x,emoticon_dict))# Use emoticon_dict to replace decode emojis in the dataset\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:16:04.733318Z","iopub.execute_input":"2021-10-10T08:16:04.73401Z","iopub.status.idle":"2021-10-10T08:16:04.863955Z","shell.execute_reply.started":"2021-10-10T08:16:04.733976Z","shell.execute_reply":"2021-10-10T08:16:04.863227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to remove the remaining emojis in the tweets\ndef remove_emoji(message):\n  \n    emoji_pattern = re.compile(\"[\"                   # Create a regular expression\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', message)\n\ndata['tidy_message'] = data['tidy_message'].apply(lambda x: remove_emoji(x))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:16:10.243952Z","iopub.execute_input":"2021-10-10T08:16:10.244631Z","iopub.status.idle":"2021-10-10T08:16:10.459073Z","shell.execute_reply.started":"2021-10-10T08:16:10.244598Z","shell.execute_reply":"2021-10-10T08:16:10.458358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove special characters, numbers, punctuations\ndata['tidy_message'] = data['tidy_message'].str.replace(\"[^a-zA-Z#]\", \" \")\ndata.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:16:16.124232Z","iopub.execute_input":"2021-10-10T08:16:16.125051Z","iopub.status.idle":"2021-10-10T08:16:16.425365Z","shell.execute_reply.started":"2021-10-10T08:16:16.125011Z","shell.execute_reply":"2021-10-10T08:16:16.424336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing short words\ndata['tidy_message'] = data['tidy_message'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:16:27.073584Z","iopub.execute_input":"2021-10-10T08:16:27.07388Z","iopub.status.idle":"2021-10-10T08:16:27.204574Z","shell.execute_reply.started":"2021-10-10T08:16:27.07385Z","shell.execute_reply":"2021-10-10T08:16:27.203823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenization\n\ntokenized_tweet = data['tidy_message'].apply(lambda x: x.split()) #Tokenize the dataset\ntokenized_tweet.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:16:33.905748Z","iopub.execute_input":"2021-10-10T08:16:33.906062Z","iopub.status.idle":"2021-10-10T08:16:33.958286Z","shell.execute_reply.started":"2021-10-10T08:16:33.906032Z","shell.execute_reply":"2021-10-10T08:16:33.957193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stemming\n\n\n#Use PorterStemmer() to strip suffixes from the words\nfrom nltk.stem.porter import * #Import * from nltk.stem.porter\nstemmer = PorterStemmer() #Initialize an instance of PorterStemmer and assign it a variable 'stemmer'\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) #Stem the dataset\ntokenized_tweet.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:16:37.711627Z","iopub.execute_input":"2021-10-10T08:16:37.71201Z","iopub.status.idle":"2021-10-10T08:16:47.297832Z","shell.execute_reply.started":"2021-10-10T08:16:37.71198Z","shell.execute_reply":"2021-10-10T08:16:47.296714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n#We will now bring the tokens back together\nfor i in range(len(tokenized_tweet)): #Loop through token list of stems\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i]) #Join the list into a string of characters\n\ndata['tidy_message'] = tokenized_tweet #Assign the string of words to the dataset in the tid_message column\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:16:47.300355Z","iopub.execute_input":"2021-10-10T08:16:47.30074Z","iopub.status.idle":"2021-10-10T08:16:47.651605Z","shell.execute_reply.started":"2021-10-10T08:16:47.300675Z","shell.execute_reply":"2021-10-10T08:16:47.65076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Split the dataset back to the training set and the testing set\ntrain = data[:len(train_df)] #Split to the lenght of original training set\ntest = data[len(train_df):]  #Split to the length of original testing set\nprint(train.shape)\nprint(train['sentiment'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:16:51.782046Z","iopub.execute_input":"2021-10-10T08:16:51.782779Z","iopub.status.idle":"2021-10-10T08:16:51.790763Z","shell.execute_reply.started":"2021-10-10T08:16:51.782746Z","shell.execute_reply":"2021-10-10T08:16:51.789575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a new length value column that contains the lengths of the messages\ntrain['message_length'] = train['message'].apply(len)\ntrain['message_length'].groupby(train['sentiment']).describe()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:17:32.86428Z","iopub.execute_input":"2021-10-10T08:17:32.865134Z","iopub.status.idle":"2021-10-10T08:17:32.906282Z","shell.execute_reply.started":"2021-10-10T08:17:32.865078Z","shell.execute_reply":"2021-10-10T08:17:32.905328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_words =' '.join([text for text in data['tidy_message'][data['sentiment'] == 1]]) #Words in the positve class\nnegative_words = ' '.join([text for text in data['tidy_message'][data['sentiment'] == -1]]) #Words in negative class\nnormal_words =' '.join([text for text in data['tidy_message'][data['sentiment'] == 0]]) #Words in the neutral class\nnews_words =' '.join([text for text in data['tidy_message'][data['sentiment'] == 2]]) #Words in the news class","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:17:00.984996Z","iopub.execute_input":"2021-10-10T08:17:00.985308Z","iopub.status.idle":"2021-10-10T08:17:01.000645Z","shell.execute_reply.started":"2021-10-10T08:17:00.985276Z","shell.execute_reply":"2021-10-10T08:17:00.999626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a function to collect hashtags\ndef hashtag_extract(x):\n    hashtags = [] #Initialize an empty list\n    for i in x:   #Loop over the words in the tweet\n        ht = re.findall(r\"#(\\w+)\", i) #Create a regular expression to get the hashtags in a tweet\n        hashtags.append(ht) #Add all those hashtags to the empty hashtag list\n    return hashtags\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:17:53.620184Z","iopub.execute_input":"2021-10-10T08:17:53.620821Z","iopub.status.idle":"2021-10-10T08:17:53.625962Z","shell.execute_reply.started":"2021-10-10T08:17:53.620781Z","shell.execute_reply":"2021-10-10T08:17:53.625168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# extracting hashtags from the news\nHT_news = hashtag_extract(data['tidy_message'][data['sentiment'] == 2])\n# extracting hashtags from positive sentiments\nHT_positive = hashtag_extract(data['tidy_message'][data['sentiment'] == 1])\n# extract hashtags from neutral sentiments\nHT_normal = hashtag_extract(data['tidy_message'][data['sentiment'] == 0])\n# extracting hashtags from negative sentiments\nHT_negative = hashtag_extract(data['tidy_message'][data['sentiment'] == -1])\n\n# unnesting list of all sentiments\nHT_news = sum(HT_news,[])\nHT_positive = sum(HT_positive,[])\nHT_normal = sum(HT_normal,[])\nHT_negative = sum(HT_negative,[])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:17:59.416583Z","iopub.execute_input":"2021-10-10T08:17:59.417209Z","iopub.status.idle":"2021-10-10T08:17:59.513975Z","shell.execute_reply.started":"2021-10-10T08:17:59.417176Z","shell.execute_reply":"2021-10-10T08:17:59.513252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nX = train['tidy_message'] #X is the features of the cleaned tweets\ny = train['sentiment']    #Y is the target variable which is the train sentiment\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) #Splitting train set into training and testing data\n#Print out the shape of the training set and the testing set\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:17:16.415833Z","iopub.execute_input":"2021-10-10T08:17:16.416111Z","iopub.status.idle":"2021-10-10T08:17:16.429568Z","shell.execute_reply.started":"2021-10-10T08:17:16.416082Z","shell.execute_reply":"2021-10-10T08:17:16.428481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import and call the TFidfVectorizer \nfrom sklearn.feature_extraction.text import TfidfVectorizer #Import TFidfVectorizer from sklearn\ntfidf = TfidfVectorizer()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:18:57.146604Z","iopub.execute_input":"2021-10-10T08:18:57.147502Z","iopub.status.idle":"2021-10-10T08:18:57.152307Z","shell.execute_reply.started":"2021-10-10T08:18:57.147465Z","shell.execute_reply":"2021-10-10T08:18:57.151284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import CountVectorizer and call it\nfrom sklearn.feature_extraction.text import CountVectorizer #Import CountVectorizer from sklearn\n\ncf= CountVectorizer() ","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:19:00.619587Z","iopub.execute_input":"2021-10-10T08:19:00.620477Z","iopub.status.idle":"2021-10-10T08:19:00.624442Z","shell.execute_reply.started":"2021-10-10T08:19:00.620441Z","shell.execute_reply":"2021-10-10T08:19:00.623539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n# create a pipeline and fit it with a Logistic Regression\nfrom sklearn.linear_model import LogisticRegression #Import Logistic Regression from sklearn\n\nmodel = LogisticRegression(multi_class='ovr') #Call the Logistic Regression model and assign it to the variable 'model'\n\nclf = Pipeline([('tfidf', tfidf), ('clf', model)]) #Create a pipeline with the TF-IDF Vectorizer with the logistic model\n\n\nclf.fit(X_train, y_train) #Fit the training data to the pipeline\n\ny_pred= clf.predict(X_test) #Make predictions and assign the predictions to y_pred\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\nprint('f1_score %s' % metrics.f1_score(y_test,y_pred,average='weighted')) #Print the weighted f1 score\nprint(classification_report(y_test, y_pred)) #Classification","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:19:04.853205Z","iopub.execute_input":"2021-10-10T08:19:04.853891Z","iopub.status.idle":"2021-10-10T08:19:07.159873Z","shell.execute_reply.started":"2021-10-10T08:19:04.853857Z","shell.execute_reply":"2021-10-10T08:19:07.158816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC #Import LinearSVC from sklearn \n\nclassifier = LinearSVC() #Call LinearSVC and assign the variable 'classifier'\n\nclf = Pipeline([('tfidf', tfidf), ('clf', classifier)]) #Create a pipeline with the tdidf\n\nclf.fit(X_train, y_train) #Fit the model\ny_pred = clf.predict(X_test) #Make predictions and assign the variable 'y_pred'\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\nprint('f1_score %s' % metrics.f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\nprint(classification_report(y_test, y_pred)) #Print the classification report","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:19:13.603822Z","iopub.execute_input":"2021-10-10T08:19:13.604575Z","iopub.status.idle":"2021-10-10T08:19:14.14306Z","shell.execute_reply.started":"2021-10-10T08:19:13.604542Z","shell.execute_reply":"2021-10-10T08:19:14.142041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n## create a pipeline and fit it with a  Support Vector Classifier\nfrom sklearn.svm import SVC #Import SVC from sklearn \n\nclassifier = SVC(kernel='rbf') #Call the SVC with the kernel='rbf' parameter\n\nclf = Pipeline([('tfidf', tfidf), ('clf', classifier)]) #Add the SVC model to the pipeline\n\nclf.fit(X_train, y_train) #Fit the training data\ny_pred = clf.predict(X_test) #Make predictions to the test set and assign the variable 'y_pred'\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\nprint('f1_score %s' % metrics.f1_score(y_test,y_pred,average='weighted')) #Print the f1 score\nprint(classification_report(y_test, y_pred)) #Print out the classification\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:20:05.088733Z","iopub.execute_input":"2021-10-10T08:20:05.089114Z","iopub.status.idle":"2021-10-10T08:20:42.071595Z","shell.execute_reply.started":"2021-10-10T08:20:05.089083Z","shell.execute_reply":"2021-10-10T08:20:42.07076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n#Create a barplot for the train dataset classes\nNews = train['sentiment'].value_counts()[2] #Take values of class 2 and assign the variable 'News'\nPro= train['sentiment'].value_counts()[1]   #Take values of class 1 and assign the variabe 'Pro'\nNeutral=train['sentiment'].value_counts()[0]#Take the values of class 0 and assign the variable 'Neutral'\nAnti=train['sentiment'].value_counts()[-1]  #Take the values of class -1 and assing the variable 'Anti'\n\nsns.barplot(['News ','Pro','Neutral','Anti'],[News,Pro,Neutral,Anti]) #Use seaborn barplot and add a list of classes\nplt.xlabel('Tweet Classification') #X-label of the data\nplt.ylabel('Count of Tweets')      #Y_label of the data\nplt.title('Dataset labels distribution') #Give the data a title 'Dataset lables distribution'\nplt.show() #Display the dataset\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:23:22.92183Z","iopub.execute_input":"2021-10-10T08:23:22.922418Z","iopub.status.idle":"2021-10-10T08:23:23.16109Z","shell.execute_reply.started":"2021-10-10T08:23:22.922384Z","shell.execute_reply":"2021-10-10T08:23:23.160257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import resample\n#Downsample and upsample train dataset\ndf_majority = train[train.sentiment==1] #Create a new dataframe of the majority pro class\ndf_minority = train[train.sentiment==0] #Create a new dataframefor the minority neutral class\ndf_minority1 = train[train.sentiment==2] #Create a dataframe for the news class\ndf_minority2 = train[train.sentiment==-1]#Create a dataframe for the anti class\n\n# Downsample majority class\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=5000,     # Using a benchmark of 3640\n                                 random_state=123) # reproducible results\n#Upsampling the least minority class\ndf_minority_up = resample(df_minority, \n                        replace=True,    # sample without replacement\n                        n_samples=5000,     # to match the second majority class\n                        random_state=123) # reproducible results\n\ndf_minority_up1 = resample(df_minority1, \n                        replace=True,    # sample without replacement\n                        n_samples=5000,     # to match the second majority class\n                        random_state=123) # reproducible results\n\ndf_minority_up2 = resample(df_minority2, \n                        replace=True,    # sample without replacement\n                        n_samples=5000,     # to match the second majority class\n                        random_state=123) # reproducible results\n\n# Combine minority class with downsampled majority class\ndf_resampled = pd.concat([df_majority_downsampled,df_minority_up,df_minority_up1, df_minority_up2])\n \n# Display new class counts\ndf_resampled.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:24:15.252793Z","iopub.execute_input":"2021-10-10T08:24:15.2531Z","iopub.status.idle":"2021-10-10T08:24:15.285067Z","shell.execute_reply.started":"2021-10-10T08:24:15.25307Z","shell.execute_reply":"2021-10-10T08:24:15.284194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_resampled['message']\ny = df_resampled['sentiment']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:25:01.90682Z","iopub.execute_input":"2021-10-10T08:25:01.907115Z","iopub.status.idle":"2021-10-10T08:25:01.918647Z","shell.execute_reply.started":"2021-10-10T08:25:01.907085Z","shell.execute_reply":"2021-10-10T08:25:01.917663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install PyJWT==1.7.1","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:33:25.363968Z","iopub.execute_input":"2021-10-10T08:33:25.364272Z","iopub.status.idle":"2021-10-10T08:35:55.513555Z","shell.execute_reply.started":"2021-10-10T08:33:25.364242Z","shell.execute_reply":"2021-10-10T08:35:55.512193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# create a pipeline and fit it with a Logistic Regression\nfrom sklearn.svm import LinearSVC \nfrom sklearn.linear_model import LogisticRegression #Import logistic regression model from sklearn\n\nmodel = LogisticRegression(C=50,multi_class='ovr',solver='liblinear') #Call logistic regression model and assign variable 'model'\n\nclf_sam = Pipeline([('tfidf', tfidf), ('clf', model)]) #Create a pipeline with the logistic model and tf-idf vectorizer\n\n\nclf_sam.fit(X_train, y_train) #Fit the training set\n\ny_pred= clf_sam.predict(X_test) #Fit the test set\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test)) #Print accuracy\nprint('f1_score %s' % metrics.f1_score(y_test,y_pred,average='weighted')) #Print f1 score\nprint(classification_report(y_test, y_pred)) #Print classification report\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T08:36:03.233238Z","iopub.execute_input":"2021-10-10T08:36:03.233459Z","iopub.status.idle":"2021-10-10T08:36:05.092444Z","shell.execute_reply.started":"2021-10-10T08:36:03.233431Z","shell.execute_reply":"2021-10-10T08:36:05.091436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## create a pipeline and fit it with a  Support Vector Classifier\nfrom sklearn.svm import SVC #Import SVC from sklearn \n\nclassifier = SVC(kernel='rbf') #Call the SVC with the kernel='rbf' parameter\n\nclf_rbf = Pipeline([('tfidf', tfidf), ('clf', classifier)]) #Add the SVC model to the pipeline\n\nclf_rbf.fit(X_train, y_train) #Fit the training data\ny_pred = clf_rbf.predict(X_test) #Make predictions to the test set and assign the variable 'y_pred'\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\nprint('f1_score %s' % metrics.f1_score(y_test,y_pred,average='weighted')) #Print the f1 score\nprint(classification_report(y_test, y_pred)) #Print out the classification","metadata":{"execution":{"iopub.status.busy":"2021-10-10T09:05:29.60277Z","iopub.execute_input":"2021-10-10T09:05:29.604835Z","iopub.status.idle":"2021-10-10T09:06:34.0196Z","shell.execute_reply.started":"2021-10-10T09:05:29.604702Z","shell.execute_reply":"2021-10-10T09:06:34.018532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"/kaggle/working/MagdaMostert002> Download File </a>\n         \n         ","metadata":{}},{"cell_type":"code","source":"test_x = test['message'] \ny_pred = clf_rbf.predict(test_x) \ntest['sentiment'] = y_pred \ntest['sentiment'] = test['sentiment'].astype(int) #Change the datatype of the submission\nfinal =test[['tweetid', 'sentiment']]\nfinal.to_csv('/kaggle/working/MagdaMostert002.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T09:24:06.847401Z","iopub.execute_input":"2021-10-10T09:24:06.847747Z","iopub.status.idle":"2021-10-10T09:24:28.797196Z","shell.execute_reply.started":"2021-10-10T09:24:06.8477Z","shell.execute_reply":"2021-10-10T09:24:28.79625Z"},"trusted":true},"execution_count":null,"outputs":[]}]}